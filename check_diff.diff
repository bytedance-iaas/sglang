diff --git a/python/sglang/srt/layers/moe/cutlass_moe.py b/python/sglang/srt/layers/moe/cutlass_moe.py
index 6435e13d..96f84800 100644
--- a/python/sglang/srt/layers/moe/cutlass_moe.py
+++ b/python/sglang/srt/layers/moe/cutlass_moe.py
@@ -6,8 +6,6 @@ import torch
 
 from vllm import _custom_ops as ops # TODO
 from sgl_kernel import silu_and_mul
-DEEPSEEK_TP8_OP1_LABEL = 112121 
-DEEPSEEK_TP8_OP2_LABEL = 142121 
 
 # Functions that require adjustment for integrating vLLMâ€™s code into sglang.
 # ops.scaled_fp8_quant() -> scaled_fp8_quant # ref: fused_moe()
@@ -234,7 +232,7 @@ def cutlass_moe_fp8(
     #                    ab_strides1, c_strides1)
     cutlass_moe_mm(c1, rep_a_q, w1_q, rep_a1_scales, w1_scale,
                        expert_offsets[:-1], problem_sizes1, ab_strides1,
-                       ab_strides1, c_strides1, DEEPSEEK_TP8_OP1_LABEL)
+                       ab_strides1, c_strides1)
 
 
     intermediate = torch.empty((m * topk, n), device=device, dtype=out_dtype)
@@ -282,7 +280,7 @@ def cutlass_moe_fp8(
     #                    ab_strides2, c_strides2)
     cutlass_moe_mm(c2, intemediate_q, w2_q, a2_scale, w2_scale,
                        expert_offsets[:-1], problem_sizes2, ab_strides2,
-                       ab_strides2, c_strides2, DEEPSEEK_TP8_OP2_LABEL)
+                       ab_strides2, c_strides2)
     # Gather tokens
     c2 = c2[c_map].view(m, topk, k)
     if not apply_router_weight_on_input:
diff --git a/sgl-kernel/csrc/common_extension.cc b/sgl-kernel/csrc/common_extension.cc
index 1d157184..608d6393 100644
--- a/sgl-kernel/csrc/common_extension.cc
+++ b/sgl-kernel/csrc/common_extension.cc
@@ -200,7 +200,7 @@ TORCH_LIBRARY_FRAGMENT(sgl_kernel, m) {
       "cutlass_moe_mm(Tensor! out_tensors, Tensor a_tensors, Tensor b_tensors, "
       "               Tensor a_scales, Tensor b_scales, Tensor expert_offsets, "
       "               Tensor problem_sizes, Tensor a_strides, "
-      "               Tensor b_strides, Tensor c_strides, int force_kernel_id) -> ()");
+      "               Tensor b_strides, Tensor c_strides) -> ()");
   m.impl("cutlass_moe_mm", torch::kCUDA, &cutlass_moe_mm);
 
   /*
diff --git a/sgl-kernel/csrc/quantization/cutlass_w8a8/moe/grouped_mm_c3x.cu b/sgl-kernel/csrc/quantization/cutlass_w8a8/moe/grouped_mm_c3x.cu
index 1fd1342a..2b8bc3fb 100644
--- a/sgl-kernel/csrc/quantization/cutlass_w8a8/moe/grouped_mm_c3x.cu
+++ b/sgl-kernel/csrc/quantization/cutlass_w8a8/moe/grouped_mm_c3x.cu
@@ -78,125 +78,13 @@ struct sm90_fp8_config_N8192 {
                             KernelSchedule, EpilogueSchedule>;
 };
 
-
-template <typename InType, typename OutType,
-          template <typename, typename, typename> typename Epilogue>
-struct sm90_fp8_config_K7168 {
-  // N in [8192, inf)
-  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());
-  using KernelSchedule =
-      cutlass::gemm::KernelPtrArrayTmaWarpSpecializedPingpongFP8FastAccum;
-  using EpilogueSchedule =
-      cutlass::epilogue::PtrArrayTmaWarpSpecializedPingpong;
-  using TileShape = cute::Shape<cute::_64, cute::_64, cute::_128>;
-  using ClusterShape = cute::Shape<cute::_1, cute::_1, cute::_1>;
-
-  using Cutlass3xGemm =
-      cutlass_3x_group_gemm<InType, OutType, Epilogue, TileShape, ClusterShape,
-                            KernelSchedule, EpilogueSchedule>;
-};
-
-
-template <typename InType, typename OutType,
-          template <typename, typename, typename> typename Epilogue>
-struct sm90_fp8_config_K256 {
-  // N in [8192, inf)
-  static_assert(std::is_same<InType, cutlass::float_e4m3_t>());
-  using KernelSchedule =
-      cutlass::gemm::KernelPtrArrayTmaWarpSpecializedPingpongFP8FastAccum;
-  using EpilogueSchedule =
-      cutlass::epilogue::PtrArrayTmaWarpSpecializedPingpong;
-  using TileShape = cute::Shape<cute::_64, cute::_128, cute::_128>;
-  using ClusterShape = cute::Shape<cute::_1, cute::_1, cute::_1>;
-
-  using Cutlass3xGemm =
-      cutlass_3x_group_gemm<InType, OutType, Epilogue, TileShape, ClusterShape,
-                            KernelSchedule, EpilogueSchedule>;
-};
-
-
-#define JOIN_STRUCT_NAME(m, n, k, a, b, c) \
-    sm90_fp8_config##_##m##_##n##_##k##_##a##_##b##_##c
-
-#define JOIN_STRUCT_NAME_CO(m, n, k, a, b, c) \
-    sm90_fp8_co_config##_##m##_##n##_##k##_##a##_##b##_##c
-
-#define GENERATE_SM90_FP8_CONFIG(M, N, K, A, B, C) \
-template <typename InType, typename OutType, \
-          template <typename, typename, typename> typename Epilogue> \
-struct JOIN_STRUCT_NAME(M, N, K, A, B, C) { \
-  static_assert(std::is_same<InType, cutlass::float_e4m3_t>()); \
-  using KernelSchedule = \
-      cutlass::gemm::KernelPtrArrayTmaWarpSpecializedPingpongFP8FastAccum; \
-  using EpilogueSchedule = \
-      cutlass::epilogue::PtrArrayTmaWarpSpecializedPingpong; \
-  using TileShape = cute::Shape<cute::Int<M>, cute::Int<N>, cute::Int<K>>; \
-  using ClusterShape = cute::Shape<cute::Int<A>, cute::Int<B>, cute::Int<C>>; \
-  \
-  using Cutlass3xGemm = \
-      cutlass_3x_group_gemm<InType, OutType, Epilogue, TileShape, ClusterShape, \
-                            KernelSchedule, EpilogueSchedule>; \
-};
-
-#define GENERATE_SM90_FP8_CO_CONFIG(M, N, K, A, B, C) \
-template <typename InType, typename OutType, \
-          template <typename, typename, typename> typename Epilogue> \
-struct JOIN_STRUCT_NAME_CO(M, N, K, A, B, C) { \
-  static_assert(std::is_same<InType, cutlass::float_e4m3_t>()); \
-  using KernelSchedule = \
-      cutlass::gemm::KernelPtrArrayTmaWarpSpecializedCooperativeFP8FastAccum; \
-  using EpilogueSchedule = \
-      cutlass::epilogue::PtrArrayTmaWarpSpecializedCooperative; \
-  using TileShape = cute::Shape<cute::Int<M>, cute::Int<N>, cute::Int<K>>; \
-  using ClusterShape = cute::Shape<cute::Int<A>, cute::Int<B>, cute::Int<C>>; \
-  \
-  using Cutlass3xGemm = \
-      cutlass_3x_group_gemm<InType, OutType, Epilogue, TileShape, ClusterShape, \
-                            KernelSchedule, EpilogueSchedule>; \
-};
-
-GENERATE_SM90_FP8_CONFIG(64,64,128,1,1,1)
-GENERATE_SM90_FP8_CONFIG(64,64,128,1,1,2)
-GENERATE_SM90_FP8_CONFIG(64,64,128,1,2,1)
-GENERATE_SM90_FP8_CONFIG(64,64,128,1,2,2)
-GENERATE_SM90_FP8_CONFIG(64,64,128,2,1,1)
-GENERATE_SM90_FP8_CONFIG(64,64,128,2,1,2)
-GENERATE_SM90_FP8_CONFIG(64,64,128,2,2,1)
-GENERATE_SM90_FP8_CONFIG(64,64,128,2,2,2)
-GENERATE_SM90_FP8_CONFIG(64,256,128,1,1,1)
-GENERATE_SM90_FP8_CONFIG(64,256,128,1,1,2)
-GENERATE_SM90_FP8_CONFIG(64,256,128,1,2,1)
-GENERATE_SM90_FP8_CONFIG(64,256,128,1,2,2)
-GENERATE_SM90_FP8_CONFIG(64,256,128,2,1,1)
-GENERATE_SM90_FP8_CONFIG(64,256,128,2,1,2)
-GENERATE_SM90_FP8_CONFIG(64,256,128,2,2,1)
-GENERATE_SM90_FP8_CONFIG(64,256,128,2,2,2)
-
-GENERATE_SM90_FP8_CO_CONFIG(128,64,128,1,1,1)
-GENERATE_SM90_FP8_CO_CONFIG(128,64,128,1,1,2)
-GENERATE_SM90_FP8_CO_CONFIG(128,64,128,1,2,1)
-GENERATE_SM90_FP8_CO_CONFIG(128,64,128,1,2,2)
-GENERATE_SM90_FP8_CO_CONFIG(128,64,128,2,1,1)
-GENERATE_SM90_FP8_CO_CONFIG(128,64,128,2,1,2)
-GENERATE_SM90_FP8_CO_CONFIG(128,64,128,2,2,1)
-GENERATE_SM90_FP8_CO_CONFIG(128,64,128,2,2,2)
-GENERATE_SM90_FP8_CO_CONFIG(128,256,128,1,1,1)
-GENERATE_SM90_FP8_CO_CONFIG(128,256,128,1,1,2)
-GENERATE_SM90_FP8_CO_CONFIG(128,256,128,1,2,1)
-GENERATE_SM90_FP8_CO_CONFIG(128,256,128,1,2,2)
-GENERATE_SM90_FP8_CO_CONFIG(128,256,128,2,1,1)
-GENERATE_SM90_FP8_CO_CONFIG(128,256,128,2,1,2)
-GENERATE_SM90_FP8_CO_CONFIG(128,256,128,2,2,1)
-GENERATE_SM90_FP8_CO_CONFIG(128,256,128,2,2,2)
-
-
 template <typename InType, typename OutType>
 void run_cutlass_moe_mm_sm90(
     torch::Tensor& out_tensors, torch::Tensor const& a_tensors,
     torch::Tensor const& b_tensors, torch::Tensor const& a_scales,
     torch::Tensor const& b_scales, torch::Tensor const& expert_offsets,
     torch::Tensor const& problem_sizes, torch::Tensor const& a_strides,
-    torch::Tensor const& b_strides, torch::Tensor const& c_strides, int64_t force_kernel_id) {
+    torch::Tensor const& b_strides, torch::Tensor const& c_strides) {
   TORCH_CHECK(a_tensors.size(0) > 0, "No input A tensors provided.");
   TORCH_CHECK(b_tensors.size(0) > 0, "No input B tensors provided.");
   TORCH_CHECK(out_tensors.size(0) > 0, "No output tensors provided.");
@@ -218,331 +106,44 @@ void run_cutlass_moe_mm_sm90(
   using Cutlass3xGemmDefault = typename sm90_fp8_config_default<
       InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;
 
-  using Cutlass3xGemmK7168 = typename sm90_fp8_config_K7168<
-      InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;
-  using Cutlass3xGemmK256 = typename sm90_fp8_config_K256<
-      InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;
-
   uint32_t const m = a_tensors.size(0);
   uint32_t const n = out_tensors.size(1);
   uint32_t const k = a_tensors.size(1);
-  if (force_kernel_id >=0 ) {
-      switch (force_kernel_id) {
-        case 112111: {
-            using Cutlass3xGemmKSelected = typename JOIN_STRUCT_NAME(64,64,128,1,1,1)<InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;
-            cutlass_group_gemm_caller<Cutlass3xGemmKSelected>(
-                out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-                problem_sizes, a_strides, b_strides, c_strides);
-            break;
-        }
-        case 112112: {
-            using Cutlass3xGemmKSelected = typename JOIN_STRUCT_NAME(64,64,128,1,1,2)<InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;
-            cutlass_group_gemm_caller<Cutlass3xGemmKSelected>(
-                out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-                problem_sizes, a_strides, b_strides, c_strides);
-            break;
-        }
-        case 112121: {
-            using Cutlass3xGemmKSelected = typename JOIN_STRUCT_NAME(64,64,128,1,2,1)<InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;
-            cutlass_group_gemm_caller<Cutlass3xGemmKSelected>(
-                out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-                problem_sizes, a_strides, b_strides, c_strides);
-            break;
-        }
-        case 112122: {
-            using Cutlass3xGemmKSelected = typename JOIN_STRUCT_NAME(64,64,128,1,2,2)<InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;
-            cutlass_group_gemm_caller<Cutlass3xGemmKSelected>(
-                out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-                problem_sizes, a_strides, b_strides, c_strides);
-            break;
-        }
-        case 112211: {
-            using Cutlass3xGemmKSelected = typename JOIN_STRUCT_NAME(64,64,128,2,1,1)<InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;
-            cutlass_group_gemm_caller<Cutlass3xGemmKSelected>(
-                out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-                problem_sizes, a_strides, b_strides, c_strides);
-            break;
-        }
-        case 112212: {
-            using Cutlass3xGemmKSelected = typename JOIN_STRUCT_NAME(64,64,128,2,1,2)<InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;
-            cutlass_group_gemm_caller<Cutlass3xGemmKSelected>(
-                out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-                problem_sizes, a_strides, b_strides, c_strides);
-            break;
-        }
-        case 112221: {
-            using Cutlass3xGemmKSelected = typename JOIN_STRUCT_NAME(64,64,128,2,2,1)<InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;
-            cutlass_group_gemm_caller<Cutlass3xGemmKSelected>(
-                out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-                problem_sizes, a_strides, b_strides, c_strides);
-            break;
-        }
-        case 112222: {
-            using Cutlass3xGemmKSelected = typename JOIN_STRUCT_NAME(64,64,128,2,2,2)<InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;
-            cutlass_group_gemm_caller<Cutlass3xGemmKSelected>(
-                out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-                problem_sizes, a_strides, b_strides, c_strides);
-            break;
-        }
-        case 142111: {
-            using Cutlass3xGemmKSelected = typename JOIN_STRUCT_NAME(64,256,128,1,1,1)<InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;
-            cutlass_group_gemm_caller<Cutlass3xGemmKSelected>(
-                out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-                problem_sizes, a_strides, b_strides, c_strides);
-            break;
-        }
-        case 142112: {
-            using Cutlass3xGemmKSelected = typename JOIN_STRUCT_NAME(64,256,128,1,1,2)<InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;
-            cutlass_group_gemm_caller<Cutlass3xGemmKSelected>(
-                out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-                problem_sizes, a_strides, b_strides, c_strides);
-            break;
-        }
-        case 142121: {
-            using Cutlass3xGemmKSelected = typename JOIN_STRUCT_NAME(64,256,128,1,2,1)<InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;
-            cutlass_group_gemm_caller<Cutlass3xGemmKSelected>(
-                out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-                problem_sizes, a_strides, b_strides, c_strides);
-            break;
-        }
-        case 142122: {
-            using Cutlass3xGemmKSelected = typename JOIN_STRUCT_NAME(64,256,128,1,2,2)<InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;
-            cutlass_group_gemm_caller<Cutlass3xGemmKSelected>(
-                out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-                problem_sizes, a_strides, b_strides, c_strides);
-            break;
-        }
-        case 142211: {
-            using Cutlass3xGemmKSelected = typename JOIN_STRUCT_NAME(64,256,128,2,1,1)<InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;
-            cutlass_group_gemm_caller<Cutlass3xGemmKSelected>(
-                out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-                problem_sizes, a_strides, b_strides, c_strides);
-            break;
-        }
-        case 142212: {
-            using Cutlass3xGemmKSelected = typename JOIN_STRUCT_NAME(64,256,128,2,1,2)<InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;
-            cutlass_group_gemm_caller<Cutlass3xGemmKSelected>(
-                out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-                problem_sizes, a_strides, b_strides, c_strides);
-            break;
-        }
-        case 142221: {
-            using Cutlass3xGemmKSelected = typename JOIN_STRUCT_NAME(64,256,128,2,2,1)<InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;
-            cutlass_group_gemm_caller<Cutlass3xGemmKSelected>(
-                out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-                problem_sizes, a_strides, b_strides, c_strides);
-            break;
-        }
-        case 142222: {
-            using Cutlass3xGemmKSelected = typename JOIN_STRUCT_NAME(64,256,128,2,2,2)<InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;
-            cutlass_group_gemm_caller<Cutlass3xGemmKSelected>(
-                out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-                problem_sizes, a_strides, b_strides, c_strides);
-            break;
-        }
-        case 1212111: {
-        using Cutlass3xGemmKSelected = typename JOIN_STRUCT_NAME_CO(128,64,128,1,1,1)<InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;
-        cutlass_group_gemm_caller<Cutlass3xGemmKSelected>(
-            out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-            problem_sizes, a_strides, b_strides, c_strides);
-        break;
-        }
-        case 1212112: {
-            using Cutlass3xGemmKSelected = typename JOIN_STRUCT_NAME_CO(128,64,128,1,1,2)<InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;
-            cutlass_group_gemm_caller<Cutlass3xGemmKSelected>(
-                out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-                problem_sizes, a_strides, b_strides, c_strides);
-            break;
-        }
-        case 1212121: {
-            using Cutlass3xGemmKSelected = typename JOIN_STRUCT_NAME_CO(128,64,128,1,2,1)<InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;
-            cutlass_group_gemm_caller<Cutlass3xGemmKSelected>(
-                out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-                problem_sizes, a_strides, b_strides, c_strides);
-            break;
-        }
-        case 1212122: {
-            using Cutlass3xGemmKSelected = typename JOIN_STRUCT_NAME_CO(128,64,128,1,2,2)<InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;
-            cutlass_group_gemm_caller<Cutlass3xGemmKSelected>(
-                out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-                problem_sizes, a_strides, b_strides, c_strides);
-            break;
-        }
-        case 1212211: {
-            using Cutlass3xGemmKSelected = typename JOIN_STRUCT_NAME_CO(128,64,128,2,1,1)<InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;
-            cutlass_group_gemm_caller<Cutlass3xGemmKSelected>(
-                out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-                problem_sizes, a_strides, b_strides, c_strides);
-            break;
-        }
-        case 1212212: {
-            using Cutlass3xGemmKSelected = typename JOIN_STRUCT_NAME_CO(128,64,128,2,1,2)<InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;
-            cutlass_group_gemm_caller<Cutlass3xGemmKSelected>(
-                out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-                problem_sizes, a_strides, b_strides, c_strides);
-            break;
-        }
-        case 1212221: {
-            using Cutlass3xGemmKSelected = typename JOIN_STRUCT_NAME_CO(128,64,128,2,2,1)<InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;
-            cutlass_group_gemm_caller<Cutlass3xGemmKSelected>(
-                out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-                problem_sizes, a_strides, b_strides, c_strides);
-            break;
-        }
-        case 1212222: {
-            using Cutlass3xGemmKSelected = typename JOIN_STRUCT_NAME_CO(128,64,128,2,2,2)<InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;
-            cutlass_group_gemm_caller<Cutlass3xGemmKSelected>(
-                out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-                problem_sizes, a_strides, b_strides, c_strides);
-            break;
-        }
-        case 1242111: {
-            using Cutlass3xGemmKSelected = typename JOIN_STRUCT_NAME_CO(128,256,128,1,1,1)<InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;
-            cutlass_group_gemm_caller<Cutlass3xGemmKSelected>(
-                out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-                problem_sizes, a_strides, b_strides, c_strides);
-            break;
-        }
-        case 1242112: {
-            using Cutlass3xGemmKSelected = typename JOIN_STRUCT_NAME_CO(128,256,128,1,1,2)<InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;
-            cutlass_group_gemm_caller<Cutlass3xGemmKSelected>(
-                out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-                problem_sizes, a_strides, b_strides, c_strides);
-            break;
-        }
-        case 1242121: {
-            using Cutlass3xGemmKSelected = typename JOIN_STRUCT_NAME_CO(128,256,128,1,2,1)<InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;
-            cutlass_group_gemm_caller<Cutlass3xGemmKSelected>(
-                out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-                problem_sizes, a_strides, b_strides, c_strides);
-            break;
-        }
-        case 1242122: {
-            using Cutlass3xGemmKSelected = typename JOIN_STRUCT_NAME_CO(128,256,128,1,2,2)<InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;
-            cutlass_group_gemm_caller<Cutlass3xGemmKSelected>(
-                out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-                problem_sizes, a_strides, b_strides, c_strides);
-            break;
-        }
-        case 1242211: {
-            using Cutlass3xGemmKSelected = typename JOIN_STRUCT_NAME_CO(128,256,128,2,1,1)<InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;
-            cutlass_group_gemm_caller<Cutlass3xGemmKSelected>(
-                out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-                problem_sizes, a_strides, b_strides, c_strides);
-            break;
-        }
-        case 1242212: {
-            using Cutlass3xGemmKSelected = typename JOIN_STRUCT_NAME_CO(128,256,128,2,1,2)<InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;
-            cutlass_group_gemm_caller<Cutlass3xGemmKSelected>(
-                out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-                problem_sizes, a_strides, b_strides, c_strides);
-            break;
-        }
-        case 1242221: {
-            using Cutlass3xGemmKSelected = typename JOIN_STRUCT_NAME_CO(128,256,128,2,2,1)<InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;
-            cutlass_group_gemm_caller<Cutlass3xGemmKSelected>(
-                out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-                problem_sizes, a_strides, b_strides, c_strides);
-            break;
-        }
-        case 1242222: {
-            using Cutlass3xGemmKSelected = typename JOIN_STRUCT_NAME_CO(128,256,128,2,2,2)<InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;
-            cutlass_group_gemm_caller<Cutlass3xGemmKSelected>(
-                out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-                problem_sizes, a_strides, b_strides, c_strides);
-            break;
-        }
-        default : {
-            using Cutlass3xGemmKSelected = typename sm90_fp8_config_default<InType, OutType, vllm::c3x::ScaledEpilogueArray>::Cutlass3xGemm;
-            cutlass_group_gemm_caller<Cutlass3xGemmKSelected>(
-                out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-                problem_sizes, a_strides, b_strides, c_strides);
-        }
-      }
+
+  if (n >= 8192) {
+    cutlass_group_gemm_caller<Cutlass3xGemmN8192>(
+        out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
+        problem_sizes, a_strides, b_strides, c_strides);
+  } else if (k >= 8192) {
+    cutlass_group_gemm_caller<Cutlass3xGemmK8192>(
+        out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
+        problem_sizes, a_strides, b_strides, c_strides);
+  } else if (m <= 16) {
+    cutlass_group_gemm_caller<Cutlass3xGemmM16>(
+        out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
+        problem_sizes, a_strides, b_strides, c_strides);
   } else {
-    if (n >= 8192) {
-        cutlass_group_gemm_caller<Cutlass3xGemmN8192>(
-            out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-            problem_sizes, a_strides, b_strides, c_strides);
-    } else if (k >= 8192) {
-        cutlass_group_gemm_caller<Cutlass3xGemmK8192>(
-            out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-            problem_sizes, a_strides, b_strides, c_strides);
-    } else if (m <= 16) {
-        cutlass_group_gemm_caller<Cutlass3xGemmM16>(
-            out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-            problem_sizes, a_strides, b_strides, c_strides);
-    } else {
-        cutlass_group_gemm_caller<Cutlass3xGemmDefault>(
-            out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-            problem_sizes, a_strides, b_strides, c_strides);
-    }
-  } 
+    cutlass_group_gemm_caller<Cutlass3xGemmDefault>(
+        out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
+        problem_sizes, a_strides, b_strides, c_strides);
+  }
 }
 
-// bool generate_cutlass_template_args(int force_kernel_id, std::vector<int>& cluster_args, std::vector<int>& tile_args) {
-//     std::vector<std::vector<int>> c_options = {{1,2},{1,2},{1}};
-
-//     std::vector<std::vector<int>> t_options = {{64, 128, 256}, {32, 64, 128, 256}, {128, 256}};
-
-//     int max_label_value = c_options[0].size() *  1e5 +
-//                           c_options[1].size() *  1e4 +
-//                           c_options[2].size() *  1e3 +
-//                           t_options[0].size() *  1e2 +
-//                           t_options[1].size() *  1e1 +
-//                           t_options[2].size() *  1e0; 
-
-//     if ( force_kernel_id > max_label_value) {
-//         std::cerr << "invaild kernel id: " << force_kernel_id << std::endl;
-//         return false;
-//     }
-
-//     int c1,c2,c3;
-//     int t1,t2,t3;
-//     int remain = force_kernel_id;
-
-//     c1 = c_options[0][(int)(remain / (int)1e5 -1)];
-//     remain = remain % ((int)1e5);
-    
-//     c2 = c_options[1][(int)(remain / (int)1e4 -1)];
-//     remain = remain % ((int)1e4);
-
-//     c3 = c_options[2][(int)(remain / (int)1e3 -1)];
-//     remain = remain % ((int)1e3);
-
-//     t1 = t_options[0][(int)(remain / (int)1e2 -1)];
-//     remain = remain % ((int)1e2);
-    
-//     t2 = t_options[1][(int)(remain / (int)1e1 -1)];
-//     remain = remain % ((int)1e1);
-
-//     t3 = t_options[2][(int)(remain / (int)1e0 -1)];
-
-//     cluster_args = {c1, c2, c3};
-//     tile_args = {t1, t2, t3};
-//     return true;
-// }
-
 void dispatch_moe_mm_sm90(
     torch::Tensor& out_tensors, torch::Tensor const& a_tensors,
     torch::Tensor const& b_tensors, torch::Tensor const& a_scales,
     torch::Tensor const& b_scales, torch::Tensor const& expert_offsets,
     torch::Tensor const& problem_sizes, torch::Tensor const& a_strides,
-    torch::Tensor const& b_strides, torch::Tensor const& c_strides,
-    int64_t force_kernel_id ) {    
-    if (out_tensors.dtype() == torch::kBFloat16) {
-        run_cutlass_moe_mm_sm90<cutlass::float_e4m3_t, cutlass::bfloat16_t>(
-            out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-            problem_sizes, a_strides, b_strides, c_strides, force_kernel_id);
-    } else if (out_tensors.dtype() == torch::kHalf) {
-        run_cutlass_moe_mm_sm90<cutlass::float_e4m3_t, cutlass::half_t>(
-            out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-            problem_sizes, a_strides, b_strides, c_strides, force_kernel_id);
-    } else if (out_tensors.dtype() == torch::kFloat8_e4m3fn) {
-        run_cutlass_moe_mm_sm90<cutlass::float_e4m3_t, cutlass::float_e4m3_t>(
+    torch::Tensor const& b_strides, torch::Tensor const& c_strides) {
+  if (out_tensors.dtype() == torch::kBFloat16) {
+    run_cutlass_moe_mm_sm90<cutlass::float_e4m3_t, cutlass::bfloat16_t>(
+        out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
+        problem_sizes, a_strides, b_strides, c_strides);
+  } else {
+    run_cutlass_moe_mm_sm90<cutlass::float_e4m3_t, cutlass::half_t>(
         out_tensors, a_tensors, b_tensors, a_scales, b_scales, expert_offsets,
-        problem_sizes, a_strides, b_strides, c_strides, force_kernel_id);
-    }
+        problem_sizes, a_strides, b_strides, c_strides);
+  }
 }
 
 }  // namespace
@@ -552,8 +153,8 @@ void cutlass_moe_mm_sm90(
     torch::Tensor const& b_tensors, torch::Tensor const& a_scales,
     torch::Tensor const& b_scales, torch::Tensor const& expert_offsets,
     torch::Tensor const& problem_sizes, torch::Tensor const& a_strides,
-    torch::Tensor const& b_strides, torch::Tensor const& c_strides, int64_t force_kernel_id) {
-    dispatch_moe_mm_sm90(out_tensors, a_tensors, b_tensors, a_scales, b_scales,
+    torch::Tensor const& b_strides, torch::Tensor const& c_strides) {
+  dispatch_moe_mm_sm90(out_tensors, a_tensors, b_tensors, a_scales, b_scales,
                        expert_offsets, problem_sizes, a_strides, b_strides,
-                       c_strides, force_kernel_id);
+                       c_strides);
 }
diff --git a/sgl-kernel/csrc/quantization/cutlass_w8a8/moe/grouped_mm_c3x.cuh b/sgl-kernel/csrc/quantization/cutlass_w8a8/moe/grouped_mm_c3x.cuh
index d2169dbe..ad3cddfb 100644
--- a/sgl-kernel/csrc/quantization/cutlass_w8a8/moe/grouped_mm_c3x.cuh
+++ b/sgl-kernel/csrc/quantization/cutlass_w8a8/moe/grouped_mm_c3x.cuh
@@ -72,10 +72,6 @@ struct cutlass_3x_group_gemm {
   struct GemmKernel : public KernelType {};
 };
 
-// static void print_problem_shape( ProblemShape problemshape) {
-//     int grou
-// }
-
 template <typename Gemm>
 void cutlass_group_gemm_caller(
     torch::Tensor& out_tensors, torch::Tensor const& a_tensors,
@@ -94,7 +90,7 @@ void cutlass_group_gemm_caller(
   bool per_out_ch = b_scales.numel() != num_experts;
 
   auto stream = at::cuda::getCurrentCUDAStream(a_tensors.device().index());
-  
+
   auto options_int =
       torch::TensorOptions().dtype(torch::kInt64).device(a_tensors.device());
 
@@ -103,12 +99,11 @@ void cutlass_group_gemm_caller(
   torch::Tensor out_ptrs = torch::empty(num_experts, options_int);
   torch::Tensor a_scales_ptrs = torch::empty(num_experts, options_int);
   torch::Tensor b_scales_ptrs = torch::empty(num_experts, options_int);
-  
+
   run_get_group_gemm_starts(expert_offsets, a_ptrs, b_ptrs, out_ptrs,
                             a_scales_ptrs, b_scales_ptrs, a_tensors, b_tensors,
                             out_tensors, a_scales, b_scales);
-  
-  
+
   using GemmKernel = typename Gemm::GemmKernel;
   using StrideA = Stride<int64_t, Int<1>, Int<0>>;
   using StrideB = Stride<int64_t, Int<1>, Int<0>>;
@@ -118,14 +113,7 @@ void cutlass_group_gemm_caller(
       static_cast<ProblemShape::UnderlyingProblemShape*>(
           problem_sizes.data_ptr());
   ProblemShape prob_shape{num_experts, problem_sizes_as_shapes, nullptr};
-  
-//   int dim_1 = problem_sizes.size(0);
-//   int dim_2 = problem_sizes.size(1);
-// //   int dim_3 = problem_sizes.size(2);
-
-//   printf("check problem shape for moe %d <%d %d> \n", num_experts, dim_1, dim_2);
-  
-  
+
   typename GemmKernel::MainloopArguments mainloop_args{
       static_cast<const ElementAB**>(a_ptrs.data_ptr()),
       static_cast<StrideA*>(a_strides.data_ptr()),
@@ -142,22 +130,10 @@ void cutlass_group_gemm_caller(
       nullptr, static_cast<StrideC*>(c_strides.data_ptr()),
       static_cast<ElementD**>(out_ptrs.data_ptr()),
       static_cast<StrideC*>(c_strides.data_ptr())};
-  
-//   int device_id = 0;
-//   cutlass::KernelHardwareInfo kernel_hw_info = cutlass::KernelHardwareInfo::make_kernel_hardware_info<GemmKernel>(device_id);
 
   typename GemmKernel::Arguments args{
       cutlass::gemm::GemmUniversalMode::kGrouped, prob_shape, mainloop_args,
       epilogue_args};
-  
-//   auto swizzle = getenv("swizzle_size");
-//   int swizzle_num = 2;
-//   if (swizzle) {
-//     swizzle_num = atoi(swizzle);
-//     // printf("use swizzle_num = %d \n", swizzle_num);
-//   }
-  
-//   args.scheduler.max_swizzle_size = swizzle_num; 
 
   using GemmOp = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
   GemmOp gemm_op;
diff --git a/sgl-kernel/csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu b/sgl-kernel/csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu
index fccf92c7..d14eb988 100644
--- a/sgl-kernel/csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu
+++ b/sgl-kernel/csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu
@@ -178,7 +178,7 @@ void cutlass_moe_mm_sm90(
     torch::Tensor const& b_tensors, torch::Tensor const& a_scales,
     torch::Tensor const& b_scales, torch::Tensor const& expert_offsets,
     torch::Tensor const& problem_sizes, torch::Tensor const& a_strides,
-    torch::Tensor const& b_strides, torch::Tensor const& c_strides, int64_t force_kernel_id);
+    torch::Tensor const& b_strides, torch::Tensor const& c_strides);
 
 void get_cutlass_moe_mm_data_caller(
     const torch::Tensor& topk_ids, torch::Tensor& expert_offsets,
@@ -356,12 +356,12 @@ void cutlass_moe_mm(
     torch::Tensor const& b_tensors, torch::Tensor const& a_scales,
     torch::Tensor const& b_scales, torch::Tensor const& expert_offsets,
     torch::Tensor const& problem_sizes, torch::Tensor const& a_strides,
-    torch::Tensor const& b_strides, torch::Tensor const& c_strides , const int64_t force_kernel_id) {
+    torch::Tensor const& b_strides, torch::Tensor const& c_strides) {
   int32_t version_num = get_sm_version_num();
 #if defined ENABLE_CUTLASS_MOE_SM90 && ENABLE_CUTLASS_MOE_SM90
   cutlass_moe_mm_sm90(out_tensors, a_tensors, b_tensors, a_scales, b_scales,
                       expert_offsets, problem_sizes, a_strides, b_strides,
-                      c_strides, force_kernel_id);
+                      c_strides);
   return;
 #endif
   TORCH_CHECK_NOT_IMPLEMENTED(
diff --git a/sgl-kernel/include/sgl_kernel_ops.h b/sgl-kernel/include/sgl_kernel_ops.h
index 01abd358..62457c11 100644
--- a/sgl-kernel/include/sgl_kernel_ops.h
+++ b/sgl-kernel/include/sgl_kernel_ops.h
@@ -286,7 +286,7 @@ void cutlass_moe_mm(
     torch::Tensor const& b_tensors, torch::Tensor const& a_scales,
     torch::Tensor const& b_scales, torch::Tensor const& expert_offsets,
     torch::Tensor const& problem_sizes, torch::Tensor const& a_strides,
-    torch::Tensor const& b_strides, torch::Tensor const& c_strides, const int64_t force_kernel_id);
+    torch::Tensor const& b_strides, torch::Tensor const& c_strides);
 
 /*
  * From FlashInfer
diff --git a/sgl-kernel/python/sgl_kernel/get_cutlass_moe_mm_data.py b/sgl-kernel/python/sgl_kernel/get_cutlass_moe_mm_data.py
index f6217304..ed5d022f 100644
--- a/sgl-kernel/python/sgl_kernel/get_cutlass_moe_mm_data.py
+++ b/sgl-kernel/python/sgl_kernel/get_cutlass_moe_mm_data.py
@@ -37,7 +37,7 @@ def cutlass_moe_mm(out_tensors: torch.Tensor, a_tensors: torch.Tensor,
                    b_tensors: torch.Tensor, a_scales: torch.Tensor,
                    b_scales: torch.Tensor, expert_offsets: torch.Tensor,
                    problem_sizes: torch.Tensor, a_strides: torch.Tensor,
-                   b_strides: torch.Tensor, c_strides: torch.Tensor, kernel_id :int):
+                   b_strides: torch.Tensor, c_strides: torch.Tensor):
     """
     A single grouped matrix multiplication used in CUTLASS-based fused MoE.
     The function executes fp8-quantized OUT = AB matrix multiplication.
@@ -54,6 +54,6 @@ def cutlass_moe_mm(out_tensors: torch.Tensor, a_tensors: torch.Tensor,
     #                             a_strides, b_strides, c_strides)    
     torch.ops.sgl_kernel.cutlass_moe_mm.default(out_tensors, a_tensors, b_tensors, a_scales,
                                         b_scales, expert_offsets, problem_sizes,
-                                        a_strides, b_strides, c_strides, kernel_id)
+                                        a_strides, b_strides, c_strides)
 
 
